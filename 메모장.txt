cmd:
nvidia-smi : nvidia 버전확인

compile() : / loss='mse' : mean squared error / 실제 값과 예측값의 차이를 기준으로 오차를 판단하는 방식
            / optimizer='adam' : 아직 설명없음, adam이 국룰
//

batch_size : 데이터셋을 신경망에 여러 조각으로 나누어 넣을 때, 나눠 담는 데이터의 양. 디폴트 값=32

mlp = multi-layer perceptron : perceptron으로 이루어져있는 layer를 여러개 갖고 있는 형태.

//

1. data set을 읽는다.
2. 슬쩍 확인
3. isnull()로 결측치 확인. 결측치 있을 경우 (1) fillna()...
4. data encoding : pd.factorize(), replace(), LabelEncoder() ,,,
5. data normalization or standardization (데이터 정규화) : feature 값을 0~1의 값으로 바꾸거나     평균이 0, 분산이 1이 되도록 스케일링
5-1. 문제에 따라 column의 중요도? 파악
6. 데이터셋 스플릿(train, test)
7. modeling : 문제 해결에 가장 효율적인 모델 선정
8. compile, fit : 정확도(accuracy)를 올리기 위해 가장 중요하다고 표현한 사람도 있었던 optimizer(최적화 기법)를 잘 선택해야 한다...
???) 하이퍼 파라미터 튜닝을 할 때 grid search로 탐색후 진행하는 경우도 있었다. 
9. 예측, 평가 : 주로 보이는 평가 지표 confusion matrix, loss, acc, val_loss, val_acc ,,,,
10. 답안 제출

//

train_test_split() 에서 train_size 파라미터와 test_size 파라미터는 반비례되는 성질이지만, 그 합이 1이 안되어도 실행은 된다.(but, data 손실이 일어남)

plt.scatter : 점 찍기
   .plot    : 선 긋기

//

round(n) : 소수점 n 번째 자리에서 반올림

//

activation(활성화 함수) : 인공 신경망에서 입력을 변환하는 함수이다. ReLU, 시그모이드 함수, 쌍곡탄젠트 함수 등이 대표적인 활성화 함수이다.
model.fit( verbose= )   : verbose=0 : 침묵
                          verbose=1 : 디폴트
                          verbose=2 : 프로그래스바 삭제
                          verbose=나머지 : epoch 만 나옴

//

train_test_split 에서 True, False 는 0,1 로 대체 불가
fit 단계에서 validation_split으로 val데이터 분리 가능
fit 단계에서 loss의 최소값이 여러번 갱신이 되지 않을 경우, 과적합이 오기전에 최대한 loss가 낮을 때 fit을 멈추는 것을 early_stopping 이라고 한다.

//

pd.value_counts() : 갯수 세줌
이진 분류 문제에서 loss='binary_crossentropy' , output_layer에선 activaion='sigmoid' !@!@!@!!
다중 분류 문제에서 loss='categorical_crossentropy', output_layer에서 activation='softmax' @$#$@#

//

train_test_split 에서 stratify= 'target' 으로 스플릿할 때 target 값의 비율을 맞춰줄 수 있다.
softmax를 통해 나온 값과 onehotencoding한 값은 np.argmax( 값, axis=1) 를 통해 타겟 값으로 바꿔준다.

//

히든 레이어를 추가할 때 사용하던, input_dim은 행렬에서만 사용가능한 방식. 앞으로는 input_shape를 사용한다.
 (3, ) -> (1, )
 (10, 4) -> (4, )
 (100, 10, 5) -> (10, 5)
 (10000, 16, 16, 3) -> (16, 16, 3)

//

scaling을 할 때, 데이터셋을 split 하기 전보다 한 후에 하는것이 과적합을 방지해주기 때문에 성능이 좋다.

save_model(path) 를 통해 모델과 가중치를 저장할 수 있다.
load_model(path)로 저장해둔 모델과 가중치를 가져올 수 있다.
model대신 weights를 써서 가중치만 가져올 수도 있다.
ModelCheckPoint 콜백 함수로 원하는 조건의 모델을 저장할 수 있다. ( 동일한 monitor를 쓰는 early_stopping 콜백 함수에서 restore_best_weight를 True로 했을 때와 저장되는 모델이 같았다. )