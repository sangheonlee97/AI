cmd:
nvidia-smi : nvidia 버전확인

compile() : / loss='mse' : mean squared error / 실제 값과 예측값의 차이를 기준으로 오차를 판단하는 방식
            / optimizer='adam' : 아직 설명없음, adam이 국룰
//

batch_size : 데이터셋을 신경망에 여러 조각으로 나누어 넣을 때, 나눠 담는 데이터의 양. 디폴트 값=32

mlp = multi-layer perceptron : perceptron으로 이루어져있는 layer를 여러개 갖고 있는 형태.

//

1. data set을 읽는다.
2. 슬쩍 확인
3. isnull()로 결측치 확인. 결측치 있을 경우 (1) fillna()...
4. data encoding : pd.factorize(), replace(), LabelEncoder() ,,,
5. data normalization or standardization (데이터 정규화) : feature 값을 0~1의 값으로 바꾸거나     평균이 0, 분산이 1이 되도록 스케일링
5-1. 문제에 따라 column의 중요도? 파악
6. 데이터셋 스플릿(train, test)
7. modeling : 문제 해결에 가장 효율적인 모델 선정
8. compile, fit : 정확도(accuracy)를 올리기 위해 가장 중요하다고 표현한 사람도 있었던 optimizer(최적화 기법)를 잘 선택해야 한다...
???) 하이퍼 파라미터 튜닝을 할 때 grid search로 탐색후 진행하는 경우도 있었다. 
9. 예측, 평가 : 주로 보이는 평가 지표 confusion matrix, loss, acc, val_loss, val_acc ,,,,
10. 답안 제출

//

train_test_split() 에서 train_size 파라미터와 test_size 파라미터는 반비례되는 성질이지만, 그 합이 1이 안되어도 실행은 된다.(but, data 손실이 일어남)

plt.scatter : 점 찍기
   .plot    : 선 긋기

//

round(n) : 소수점 n 번째 자리에서 반올림

//

activation(활성화 함수) : 인공 신경망에서 입력을 변환하는 함수이다. ReLU, 시그모이드 함수, 쌍곡탄젠트 함수 등이 대표적인 활성화 함수이다.
model.fit( verbose= )   : verbose=0 : 침묵
                          verbose=1 : 디폴트
                          verbose=2 : 프로그래스바 삭제
                          verbose=나머지 : epoch 만 나옴

//

train_test_split 에서 True, False 는 0,1 로 대체 불가
fit 단계에서 validation_split으로 val데이터 분리 가능
fit 단계에서 loss의 최소값이 여러번 갱신이 되지 않을 경우, 과적합이 오기전에 최대한 loss가 낮을 때 fit을 멈추는 것을 early_stopping 이라고 한다.

//

pd.value_counts() : 갯수 세줌
이진 분류 문제에서 loss='binary_crossentropy' , output_layer에선 activaion='sigmoid' !@!@!@!!
다중 분류 문제에서 loss='categorical_crossentropy', output_layer에서 activation='softmax' @$#$@#

//

train_test_split 에서 stratify= 'target' 으로 스플릿할 때 target 값의 비율을 맞춰줄 수 있다.
softmax를 통해 나온 값과 onehotencoding한 값은 np.argmax( 값, axis=1) 를 통해 타겟 값으로 바꿔준다.

//

히든 레이어를 추가할 때 사용하던, input_dim은 행렬에서만 사용가능한 방식. 앞으로는 input_shape를 사용한다.
 (3, ) -> (1, )
 (10, 4) -> (4, )
 (100, 10, 5) -> (10, 5)
 (10000, 16, 16, 3) -> (16, 16, 3)

//

scaling을 할 때, 데이터셋을 split 하기 전보다 한 후에 하는것이 과적합을 방지해주기 때문에 성능이 좋다.

model.save(path) 를 통해 모델과 가중치를 저장할 수 있다.
load_model(path)로 저장해둔 모델과 가중치를 가져올 수 있다.
model대신 weights를 써서 가중치만 가져올 수도 있다. ( 가중치만 가져오기 때문에 load 할 때 model 아키텍쳐를 동일하게 만들어야 사용할 수 있다, 또 용량 차이가 난다. )
ModelCheckPoint 콜백 함수로 원하는 조건의 모델을 저장할 수 있다. ( 동일한 monitor를 쓰는 early_stopping 콜백 함수에서 restore_best_weight를 True로 했을 때와 저장되는 모델이 같았다. )

https://mkjjo.github.io/python/2019/01/10/scaler.html 스케일러 고를 때 참고

cnn:  filters, kerer_size, input_shape 
      stride(커널이 움직이는 보폭)
      
//

LabelEncoding은 1차원 값에 대한 인코딩만 되므로, X값을 인코딩할 땐 OrdinalEncoder()를 사용한다.(사용 하면 안될거 같은 주장 V)
https://abluesnake.tistory.com/169
"그러나 주의해야될 점은, Ordinal 데이터를 함부로 숫자화할 수 없다는 점입니다. 
실제로 대다수 데이터는 상하관계 혹은 피쳐 간의 차이가 샘플마다 다른 경우가 대다수입니다. 
예를 들어 A에게 상과 중의 차이가 B가 느끼는 상과 중의 차이와 다를 수 있는 것입니다. 
이렇듯 ordinal encoding을 실제로 사용하는 일은 굉장히 드뭅니다. 
특히 neural network와 같은 딥러닝에서는 ordinal한 피쳐들이 가지는 대소 혹은 상하 관계가 원핫인코딩을 통해서도 반영이 될 수 있으며,
웬만한 커다란 차원은 해결이 가능하기 때문에 더더욱 사용할 경우가 적어지고 있습니다."
https://haehwan.github.io/posts/sta-Label-ordinal/

//

from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(
    rescale=1/255.,
    horizontal_flip=True,   # 수평 뒤집기
    vertical_flip=True,     # 수직 뒤집기
    width_shift_range=0.1,  # 가로로 평행이동
    height_shift_range=0.1, # 세로로 평행이동
    rotation_range=5,       # 정해진 각도만큼 이미지를 회전
    zoom_range=1.2,         # 축소 또는 확대
    shear_range=0.7,        # 좌표 하나를 고정시키고 다른 몇 개의 좌표를 이동시키는 변환
    fill_mode='nearest',    # 빈 공간을 근사치로 채움,    
)

test_datagen = ImageDataGenerator(
    rescale=1./255,   
)

//

데이터 증폭 :   라벨값의 비중이 차이나는 경우 특정 라벨 값에 대한 예측이 어려워진다.
                그래서 데이터의 양이 적은 라벨에 증폭을 해줘서 다른 라벨 값들과 비율을 똑같이 맞춰주면 성능이 더 좋아진다.

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

//

text데이터를 Tokenizer()를 통해 학습시키려 할 때 원핫 인코딩을 안하면 라벨값에 따라 가중치가 부여되고, 원핫 인코딩을 하자니 그 가지수가 너무 커져서
값에 0이 너무 많아진다. 그래서 사용하는 방법이 임베딩이다

//

